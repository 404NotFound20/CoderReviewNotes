# 机器学习基础知识

## 基本概念

下面三个图分别是对Price的三种拟合曲线，分别记为 $f_1,f_2,f_3​$，后面概念会对这三个图详细说明。

![img](MachineLearningBasics.assets/v2-3f4959cd70308df496ecc4568a0d982d_hd.jpg)

###### 损失函数（代价函数）

假设我们拟合出关于训练数据的拟合函数 $y=f(x)$ ，那么对于给定的 $X$ ，这个函数会输出一个 $f(X)$ ，这个输出的 $f(X)$ 与真实值 $Y$ 可能是相同的，也可能是不同的，为了表示我们拟合的好坏，我们用一个函数来**度量拟合的程度**，比如： $L(Y, f(X)) = (Y - f(X))^2$ ，这个函数就称为**损失函数（Loss Function）**，或者叫**代价函数（Cost Function）**。**损失函数越小**，就代表**拟合的越好**。

###### 经验风险

那是不是我们的目标就只是让loss function越小越好呢？还不是。

这个时候还有一个概念叫**风险函数(risk function)**。**风险函数是损失函数的期望**，这是由于我们输入输出的 $X, Y$ 遵循一个联合分布，但是这个联合分布是未知的，所以无法计算。但是我们是有历史数据的，就是我们的训练集，关于训练集的**平均损失**称作经验风险(empirical risk)，即 $\frac{1}{N}\sum_{i=1}^{N}L(y_i, f(x_i))$ ，所以我们的目标就是求 $min\frac{1}{N}\sum_{i=1}^{N}L(y_i, f(x_i))$ ，称为**经验风险最小化**。

###### 结构风险

如果到这一步就完了的话，那我们看上面的图，那肯定是最右面的 $f_3$ 的经验风险函数最小了，因为它对历史的数据拟合的最好嘛。但是我们从图上来看肯定不是最好的，因为它**过度学习**历史数据，导致它在真正预测时效果会很不好，这种情况称为过拟合(over-fitting)。

为什么会造成这种结果？大白话说就是它的函数太复杂了，都有四次方了，这就引出了下面的概念，我们不仅要让经验风险最小化，还要让**结构风险最小化**。这个时候就定义了一个函数 $J(f)$ ，这个函数专门用来度量**模型的复杂度**，在机器学习中也叫**正则化(regularization)**。常用的有 $L_1,L_2$ 范数。

###### 目标函数

完成上面的介绍，就得到最终优化函数： 
$$
min (\frac{1}{N} \sum_{i=1}^{N} L(y_i, f(x_i)) + \lambda J(f))
$$
即**最小化经验风险和结构风险的和**，这个最优化函数就称之为**目标函数**。

结合上面的例子来分析：最左面的 $f_1$ 结构风险最小（模型结构最简单），但是经验风险最大（对历史数据拟合的最差）；最右面的 $f_3$ 经验风险最小（对历史数据拟合的最好），但是结构风险最大（模型结构最复杂）;而 $f_2$ 达到了二者的良好**平衡**，最适合用来预测未知数据集。